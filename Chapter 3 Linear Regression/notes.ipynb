{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "- It's a simple approach to supervised learning. It assumes that the dependence of $Y$ on $X_1,X_2,X_3,...,X_p$ is linear.\n",
    "- True regression functions are never linear. Although it may seem overly simplistic,linear regression is extremely useful both conceptually and practically.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Linear Regression for the advertising data\n",
    "\n",
    "Looking at the advertising data,which tells you the sales with three kind of advertising budget.\n",
    "<br><br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"advertising_data.png\" alt=\"Description of image\">\n",
    "</div>\n",
    "<br><br>\n",
    "\n",
    "There are some questions you may want to ask: \n",
    "- Is there a relationship b/w advertising budget and sales?\n",
    "- How strong is the relationship b/w advertising budget and sales?\n",
    "- Which media contributes to sales?\n",
    "- How accurately can we predict sales?\n",
    "- Is the relationship linear?\n",
    "- Is there synergy among the advertising media?\n",
    "\n",
    "## Simple linear regression using a simple predictor *X*\n",
    "\n",
    "- We assume a model\n",
    "$$Y = \\beta_{0} + \\beta_{1}X + \\epsilon,$$\n",
    " where $\\beta_{0}$ and $\\beta_{1}$ are two unknown constants that represent the *intercept* and *slope*,and are also knwon as *coefficients* or *parameters*, and $\\epsilon$ is the error term.\n",
    "- Given some estimates $\\hat{\\beta_{0}}$ and $\\hat{\\beta_{1}}$ for the model coefficients,we predict future sales using\n",
    "$$\\hat{y} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}x,$$\n",
    "where $\\hat{y}$ indicates a prediction of *Y* on the basis of *X = x*. The hat symbol denotes an estimated value.\n",
    "\n",
    "## Estimation of parameters by least squares\n",
    "\n",
    "- Let $\\hat{y_i} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}x_i$ be the prediction of *Y* based on the *i*th value of *X*. Then $e_i = y_i - \\hat{y_i}$ represent the *i*th *residual*.\n",
    "- We define *Residual Sum of Squares* (RSS) as \n",
    "$$RSS = e_{1}^2 + e_{2}^2 + ........ + e_{n}^2$$\n",
    "- The least squares approach chooses $\\beta_{0}$ and $\\beta_{1}$ to minimise *RSS*. The minimising value can be shown to be\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2},$$\n",
    "\n",
    "$$\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x},$$\n",
    "\n",
    "where $\\bar{y}$ and $\\bar{x}$ are the sample mean.\n",
    "\n",
    "## Assessing the accuracy of the Coefficient Estimates\n",
    "\n",
    "- The standard error of an estimator reflects how it varies under repeated sampling. We have\n",
    "$$\\text{SE}(\\hat{\\beta_0})^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right],$$\n",
    "$$\\text{SE}(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2},$$\n",
    "where $\\sigma^2 = Var(\\epsilon)$\n",
    "\n",
    "- We have our estimates ,now we want to know how precise are those estimates. Look at the second term,the numerator is the noise and denominator is the spread of the axis around their mean.The more the noise around the line,the less precise the estimates and the more the spread,the more the slope pinned down. In the below image,if all the points were concentrated in small x-region,there would have been a lot of variance in the slope per sample.The spread improves the precision hence. \n",
    "<br><br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img2.png\" alt=\"Description of image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These standard errors can be used to compute **confidence intervals**. A *95%* confidence interval is defined as a range of values such that with *95%* probability,the range will contain the true unknown value of the parameter.It has the form\n",
    "$$\\hat{\\beta_{1}} \\pm 2SE(\\hat{\\beta_{1}})$$\n",
    "- That is,there is a 95% chance that the interval \n",
    "$$[\\hat{\\beta_{1}} - 2SE(\\hat{\\beta_{1}}), \\hat{\\beta_{1}} + 2SE(\\hat{\\beta_{1}})]$$\n",
    "will contain the true value of $\\beta_{1}$(under a scenario where we got repeated samples like present sample.)\n",
    "- For advertising data(TV),the 95% confidence interval for $\\beta_{1}$ is [0.042,0.053]. i.e. the true slope is greater than,and TV advertising has a positive effect on sales.\n",
    "\n",
    "## Hypothesis Testing and Confidence Intervals\n",
    "\n",
    "- Standard errors can also be used to perform **hypothesis testing** on the coefficients. The most common hypothesis test involves the **null hypothesis** of <br><br>\n",
    "$H_0$ : There is no relationship between X and Y versus the alternative hypothesis <br><br>\n",
    "$H_A$ : There is some relationship between X and Y <br><br>\n",
    "\n",
    "- Mathematicly this corresponds to<br><br>\n",
    "$$H_0: \\beta_1 = 0$$\n",
    "versus\n",
    "$$H_A: \\beta_1 \\neq 0$$\n",
    "- To test the null hypothesis, we compute a **t-statistic**,given by\n",
    "$$t = \\frac{\\hat{\\beta_1} - 0}{SE(\\hat{\\beta_1})}$$\n",
    "- This will have a *t*-distribution with *n*-2 degrees of freedom,assuming $\\beta_1 = 0$.\n",
    "- Using statistical software,it is easy to compute the probability of observing any value equal |*t*| or larger.We call this probability the **p-value**. \n",
    "<br><br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img3.png\" alt=\"Description of image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to interpret this result?<br>\n",
    "The second line is measuring the effect of TV advertising on sales.It says the probability of observing the value of 17.67(*t-statistic*) under the assumption of *null hypothesis* (TV advertising has no effect on sales) is less than 10e-4 i.e. possible but very unlikely.<br>\n",
    "**Our conclusion therefore is that TV advertising has an effect on sales**\n",
    "- There is also a relationship between confidence intervals and hypothesis testing.<br>\n",
    "1. If we reject the null hypothesis,then the confidence interval constructed for that coefficent will not carry *zero*.\n",
    "2. But if we can't reject the null hypothesis,then confidence interval will contain *zero*.\n",
    "\n",
    "## Assessing the overall accuracy of the model\n",
    "\n",
    "- We compute the **Residual Standard Error**\n",
    "$$\\text{RSE} = \\sqrt{\\frac{1}{n-2}\\text{RSS}} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i - \\hat{y_i})^2}.$$\n",
    "where the **Residual sum-of-squares** is $\\text{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$.\n",
    "- **R-Squared** or fraction of variance explained is\n",
    "$$R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS},$$\n",
    "where $\\text{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2$ is the **total sum of squares.**\n",
    "- It can be shown that in this simple linear regression setting that $R^2 = r^2$ ,where *r* is the correlation between *X* and *Y*:\n",
    "$$\\text{r} = \\text{Cor}(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}$$\n",
    "<br><br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img4.png\" alt=\"Description of image\">\n",
    "</div>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- What does that $R^2$ value tells? <br>\n",
    "Using TV advertising budget we reduced the variance in sales by almost 60% .\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "- Here our model is\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon$$\n",
    "\n",
    "- We interpret $\\beta_j$ as the average effect on *Y* of a one unit increase in $X_j$, holding all the predictors fixed,in the advertising example the model becomes <br>\n",
    "**sales = $\\beta_0$ + $\\beta_1$ X radio + $\\beta_2$ X radio + $\\beta_3$ X newspaper + $\\epsilon$**\n",
    "<br><br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img5.png\" alt=\"Description of image\">\n",
    "</div>\n",
    "<br><br>\n",
    "Earlier with a single predictor it was a line but now it's a hyperplane as shown in the image above for two predictors.\n",
    "\n",
    "## Interpreting Regression Coefficients\n",
    "\n",
    "- The ideal scenario is when predictors are uncorrelated - a balanced design: <br>\n",
    "    1. Each coefficient can be estimated and tested separately. <br> \n",
    "    2. Interpretations such as *\" a unit change in $X_j$ is associated with a $\\beta_j$ change in Y, while all other variables stay fixed \"*, are possible.\n",
    "- Correlations amongst predictors cause problems: <br>\n",
    "    1. The variance of all coefficients tend to increase sometimes,dramatically. <br>\n",
    "    2. Interpretations become hazardous - when $X_j$ changes,everything else changes.\n",
    "- **Claims of causality** should be avoided for observational data.\n",
    "\n",
    "## Estimation and Prediction for Multiple Regression\n",
    "\n",
    "- Given estimates $\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_p},$ we can make predictions using the formula\n",
    "$$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + \\cdots + \\hat{\\beta_p}x_p$$\n",
    "- We estimate $\\beta_0,\\beta_1,...,\\beta_p,$ as the values that minimize the sum of squared residuals\n",
    "$$RSS = \\sum_{i = 1}^n (y_i - \\hat{y_i})^2$$\n",
    "The values that minimize *RSS* are the multiple least squares regression coefficients. <br><br>\n",
    "Following is the result of the advertising data:\n",
    "<br><br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img6.png\" alt=\"Description of image\">\n",
    "</div>\n",
    "<br><br>\n",
    "You can say that the presence of newspaper advertising may have significant on sales but not in the presence of radio and newspaper. You can see that there's a correlation between radio and newspaper, so it might be the case that radio has soaked up the effects of newspaper and no longer needed in the model.\n",
    "\n",
    "## Some important questions\n",
    "\n",
    "1. Is at least one of the predictors $X_1,X_2,...,X_p$ useful in predicting the response?\n",
    "2. Do all the predictors help to explain Y , or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?\n",
    "\n",
    "## Is atleast one predictor useful?\n",
    "For this question we can use the **F-statistic**\n",
    "$$F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} \\sim F_{p,n-p-1}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img7.png\" alt=\"Description of image\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "## Deciding on the important variable\n",
    "\n",
    "- The most direct approach is called **all subsets** or **best subsets** regression: we compute the least squares fit for all possible subsets and then choose between them based on some criterian that balances training error with model size.\n",
    "- However we often can't examine all models,since they are $2^p$ of them ; for example when p = 40 there are over a billion models! <br>\n",
    "Instead we need an automated approach that searches through a subset of them.There are two of them:\n",
    "\n",
    "### Forward Selection\n",
    "\n",
    "- Begin with a **null model** - a model that contains an intercept but no predictors.\n",
    "- Fit *p* simple linear regressions and add to the null model that variable that results in the lowest RSS.\n",
    "- Add to that model the variable that results in the lowest RSS amongst all two variable models.\n",
    "- Continue until some stopping rule is satisfied, for eg when all remaining variables have a p-value above some threshold.\n",
    "\n",
    "### Backward Selection\n",
    "\n",
    "- Start with all variables in the model.\n",
    "- Remove the variable with the largest p-value i.e. the variable that is the least statistically significant.\n",
    "- The new (p-1) variable model is fit, and the variable with largest p-value is removed.\n",
    "- Continue until a stopping rule is reached . For instance, we may stop when all remaining variables have  a significant p-value defined by some significant threshold.\n",
    "\n",
    "*NOTE: Later we discuss more systematic criteria for choosing an \"optimal\" member in the path of models produced by fwd or bwd stepwise selection.\n",
    "These include* **Mallow's $C_p$,Akaike information criteria(AIC),Bayesian Information Criteria(BIC),adjusted $R^2$** and **Cross Validation**\n",
    "\n",
    "## Other Considerations in the Regression Model\n",
    "\n",
    "### Qualitative Predictors\n",
    "- Some predictors aren't quantitative but qualitative, taking a discrete set of values.\n",
    "- These are also called **categorical** predictors or **factor variables**.\n",
    "- In the example below,in addition to the 7 quantitative variables there four qualitative variables: **gender,student status,marital status,and ethnicity**\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img8.png\" alt=\"Description of image\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This leads to the creation of **dummy variables**.\n",
    "\n",
    "## Extensions of the Linear Model\n",
    "\n",
    "### Interactions\n",
    "\n",
    "- In our previous analysis of the Adverstising data, we assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media. \n",
    "- For example,the linear model,\n",
    "$$\\hat{sales} = \\beta_{0} + \\beta_{1} * TV + \\beta_{2} * radio + \\beta_{3}*newspaper$$\n",
    "states that the average effect on sales of a one-unit increase in TV is always $\\beta_1$, reagrdless of the amount spent on radio.\n",
    "- But suppose that the spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.\n",
    "- In that case,given a fixed budget of $100,100 spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or radio.\n",
    "- In Marketing this is known as **synergy** effect, and in statistics it's reffered as **interaction** effect.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img9.png\" alt=\"Description of image\">\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the levels of either TV or radio are low,then true sales are lower than predicted by the linear model but when advertising is split between TV and radio,then the model tends to underestimate the sales.\n",
    "\n",
    "### Modelling Interactions - Advertising Data\n",
    "\n",
    "Model takes the form <br><br>\n",
    "$$sales = \\beta_0 + \\beta_1*TV + \\beta_2*radio + \\beta_3*(radio*TV) + \\epsilon $$\n",
    "$$sales = \\beta_0 + (\\beta_1 + \\beta_3*radio)*TV + \\beta_2*radio + \\epsilon$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img10.png\" alt=\"Description of image\" width = \"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- The results in this table suggests that interactions are important.\n",
    "- The p-value of the interaction term **TV*radio** is extremely low,indicating that there is string evidence for $H_A: \\beta_3 \\neq 0$\n",
    "- The $R^2$ for the interaction model is 96.8% compared to only 89.7% for the model that predicts sales using TV and radio without an interaction.\n",
    "- This means that (96.8 - 89.7)/(100 - 89.7) = 69% of the variability in sales that remains after the additive model has been explained by the interaction term.\n",
    "\n",
    "### Hierarchy \n",
    "- Sometimes it is the case that the an interaction term has very small p-value,but the associated main(here TV and radio) effects do not.\n",
    "- The heirarchy principle: <br>\n",
    "*\"If we include an interaction in a model,we should also include the main effects,even if the p-values associated with their coefficients aren't significant\"*\n",
    "- The rationale for this principle is that interactions are hard to interpret in a model without main effects - their meaning is changed.\n",
    "\n",
    "### Interactions between qualitative and quantitative variables\n",
    "Consider the credit card data set and suppose that we wish to predict balance using income(quantitative) and student(qualitative).Without an interaction term the model takes the form : <br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img11.png\" alt=\"Description of image\" width = \"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With interactions it takes the following form:\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img12.png\" alt=\"Description of image\" width = \"600\">\n",
    "</div>\n",
    "<br> <br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img13.png\" alt=\"Description of image\" width = \"600\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear effects of predictors\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img14.png\" alt=\"Description of image\" width = \"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure suggests that the model\n",
    "$$mpg = \\beta_0 + \\beta_1*horsepower + \\beta_2*horsepower^2 + \\epsilon$$\n",
    "may provide a better fit.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img15.png\" alt=\"Description of image\" width = \"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example we created an extra variable to accomodate polynomials.\n",
    "\n",
    "## Potential Problems\n",
    "When we fit a linear regression model to a particular dataset,many problems may occur.Most common among these are the following: <br><br>\n",
    " 1.Non-linearity of the response-predictor relationships.<br>\n",
    " 2.Correlation of error terms.<br>\n",
    " 3.Non-constant variance of error terms.<br>\n",
    " 4.Outliers.<br>\n",
    " 5.High-leverage points.<br>\n",
    " 6.Collinearity.<br>\n",
    "\n",
    "I have just mentioned a few but you can see section 3.3 if you want to read it in detail.<br>\n",
    "\n",
    " ### 1. Outliers\n",
    " - An outlier is a point for which $y_i$ is far from the value predicted by the model.\n",
    " <div style=\"text-align:center\">\n",
    "    <img src=\"img16.png\" alt=\"Description of image\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The red point in the above image illustrates a typical outlier.The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removal of the outlier. \n",
    "- In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction\n",
    " in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit.\n",
    "- However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed. Since the RSE is used to compute all confidence intervals and p-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit.\n",
    "- Similarly, inclusion of the outlier causes the $R^2$ to decline from 0.892 to 0.805.\n",
    "-  **Residual plots can be used to identify outliers.** In this example, the outlier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. \n",
    "- To address this problem, instead of plotting the residuals, we can plot the **studentized residuals**, computed by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers. \n",
    "- In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between -2 and 2.\n",
    "- If we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation.However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor.\n",
    "\n",
    "### 2. Non-constant Variance of Error Terms\n",
    "\n",
    "- Another important assumption of the linear regression model is that the error terms have a constant variance, $Var(\\epsilon_i) = \\sigma^2$. *The standard errors,confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.*\n",
    "- Unfortunately, it is often the case that the variances of the error terms are non-constant.\n",
    "- For instance, the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or **heteroscedasticity**, from the presence of a funnel shape in the residual plot. \n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img17.png\" alt=\"Description of image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An example is shown in the left-hand panel of Figure 3.11,in which the magnitude of the residuals tends to increase with the fitted values.\n",
    "- **One possible solution** : transform the response Y using a concave function such as *logY* or $\\sqrt()Y$. Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. \n",
    "- The right-hand panel of Figure 3.11 displays the residual plot after transforming the response using *logY* . The residuals now appear to have constant variance, though there is some evidence of a slight non-linear relationship in the data.\n",
    "- Sometimes we have a good idea of the variance of each response. For example, the *i*th response could be an average of $n_i$ raw observations. If each of these raw observations is uncorrelated with variance $\\sigma^2$, then their average has variance $\\sigma_{i}^2 = \\sigma^2/n_i$. In this case a simple remedy is to fit our model by *weighted least squares*, with weights proportional to the inverse weighted variances—i.e. $w_i = n_i$ in this case. Most linear regression software allows for observation weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Marketing Plan\n",
    " \n",
    "We now briefly return to the seven questions about the Advertising data that we set out to answer at the beginning of this chapter. <br>\n",
    "\n",
    "1. **Is there a relationship between sales and advertising budget?**\n",
    "- This question can be answered by fitting a multiple regression model of sales onto TV, radio, and newspaper, and testing the hypothesis $H_0 : \\beta_{TV} = \\beta_{radio} = \\beta_{newspaper} =0.$\n",
    "- We have seen that the F-statistic can be used to determine whether or not we should reject this null hypothesis. In this case the p-value corresponding to the F-statistic in below table is very low, indicating clear evidence of a relationship between advertising and sales.\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"img4.png\" alt=\"Description of image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **How strong is the relationship?**\n",
    "- We discussed two measures of model accuracy. \n",
    "- First,the *RSE* estimates the standard deviation of the response from the population regression line. For the Advertising data, the RSE is 1.69 units while the mean value for the response is 14.022, indicating a percentage error of roughly 12%. \n",
    "- Second, the $R^2$ statistic records the percentage of variability in the response that is explained by the predictors. The predictors explain almost 90% of the variance in sales.\n",
    "<br>\n",
    "3. **Which media are associated with sales?**\n",
    "- To answer this question, we can examine the p-values associated with each predictor’s **t-statistic**. In the multiple linear regression displayed in Table 3.4, the p-values for TV and radio are low,but the p-value for newspaper is not. This suggests that only TV and radio are related to sales. In\n",
    " <div style=\"text-align:center\">\n",
    "    <img src=\"img6.png\" alt=\"Description of image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. **How large is the association between each medium and sales?**\n",
    " - We saw that the standard error of $\\hat{\\beta_j}$ can be used to construct confidence intervals for $\\beta_j$. For the Advertising data, we can use the results in Table 3.4 to compute the 95% confidence intervals for the coefficients in a multiple regression model using all three media budgets as predictors. The confidence intervals are as follows: <br>\n",
    " **(0.043,0.049) for TV, (0.172,0.206) for radio, and (-0.013,0.011) for newspaper.** \n",
    " - The confidence intervals for TV and radio are narrow and far from zero, providing evidence that these media are related to sales. But the interval for newspaper includes zero, indicating that the variable is not statistically significant given the values of TV and radio.\n",
    " - We also know that collinearity can result in very wide standard errors. *Could collinearity be the reason that the confidence in\n",
    "terval associated with newspaper is so wide?* The **VIF scores** are 1.005,1.145, and 1.145 for TV, radio, and newspaper, suggesting no evidence of collinearity.\n",
    "- In order to assess the association of each medium individually on sales, we can perform three separate simple linear regressions. Re\n",
    "sults are shown in Tables. There is evidence of an extremely strong association between TV and sales and between radio and sales. There is evidence of a mild association between newspaper and sales, when the values of TV and radio are ignored.\n",
    "<br>\n",
    "5. **How accurately can we predict future sales?**\n",
    "- The accuracy associated with the estimate depends on whether we wish to predict anindividual response, $Y = f(X)+ \\epsilon$ , or the average response, $f(X)$. If the former, we use a prediction interval, and if the latter, we use a confidence interval. Prediction intervals will al\n",
    "ways be wider than confidence intervals because they account for the uncertainty associated with , the irreducible error.\n",
    "<br>\n",
    "6. **Is the relationship linear?**\n",
    "- Above we saw that residual plots can be used in order to identify non-linearity. If the relationships are linear, then the residual plots should display no pattern. \n",
    "- In the case of the Advertising data, we observe a non-linear effect, though this effect could also be observed in a residual plot. In the section \"Extension of linear model\", we discussed the inclusion of transformations of the predictors in the linear regression model in order to accommodate non-linear relationships.\n",
    "7. **Is there synergy among the advertising media?**\n",
    "- The standard linear regression model assumes an additive relationship between the predictors and the response. An additive model is easy to interpret because the association between each predictor and the response is unrelated to the values of the other predictors.\n",
    "- However, the additive assumption may be unrealistic for certain data sets. In above sections, we showed how to include an interaction term in the regression model in order to accommodate non-additive relationships. \n",
    "- A small p-value associated with the interaction term indicates the presence of such relationships. Including an interaction term in the model results in a substantial increase in $R^2$, from around 90% to almost 97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisations of the linear model\n",
    "\n",
    "Moving further we will discuss methods that expand the scope of linear models and how they are fit:\n",
    "- **Classification Problems**: logistic regression,support vector machines\n",
    "- **Non-linearity** :kernel smoothing,splines and generalised additive models;nearest neighbour methods\n",
    "- **Interactions** : Tree-based methods,bagging,random forests and boosting(these also capture non-linearties)\n",
    "- **Regularised fitting** : Ridge Regression and Lasso"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
