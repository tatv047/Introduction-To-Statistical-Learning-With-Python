{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "- It's a simple approach to supervised learning. It assumes that the dependence of $Y$ on $X_1,X_2,X_3,...,X_p$ is linear.\n",
    "- True regression functions are never linear. Although it may seem overly simplistic,linear regression is extremely useful both conceptually and practically.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Linear Regression for the advertising data\n",
    "\n",
    "Looking at the advertising data,which tells you the sales with three kind of advertising budget.\n",
    "<br><br>\n",
    "![advertisng data ](advertising_data.png)\n",
    "<br><br>\n",
    "\n",
    "There are some questions you may want to ask: \n",
    "- Is there a relationship b/w advertising budget and sales?\n",
    "- How strong is the relationship b/w advertising budget and sales?\n",
    "- Which media contributes to sales?\n",
    "- How accurately can we predict sales?\n",
    "- Is the relationship linear?\n",
    "- Is there synergy among the advertising media?\n",
    "\n",
    "## Simple linear regression using a simple predictor *X*\n",
    "\n",
    "- We assume a model\n",
    "$$Y = \\beta_{0} + \\beta_{1}X + \\epsilon,$$\n",
    " where $\\beta_{0}$ and $\\beta_{1}$ are two unknown constants that represent the *intercept* and *slope*,and are also knwon as *coefficients* or *parameters*, and $\\epsilon$ is the error term.\n",
    "- Given some estimates $\\hat{\\beta_{0}}$ and $\\hat{\\beta_{1}}$ for the model coefficients,we predict future sales using\n",
    "$$\\hat{y} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}x,$$\n",
    "where $\\hat{y}$ indicates a prediction of *Y* on the basis of *X = x*. The hat symbol denotes an estimated value.\n",
    "\n",
    "## Estimation of parameters by least squares\n",
    "\n",
    "- Let $\\hat{y_i} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}x_i$ be the prediction of *Y* based on the *i*th value of *X*. Then $e_i = y_i - \\hat{y_i}$ represent the *i*th *residual*.\n",
    "- We define *Residual Sum of Squares* (RSS) as \n",
    "$$RSS = e_{1}^2 + e_{2}^2 + ........ + e_{n}^2$$\n",
    "- The least squares approach chooses $\\beta_{0}$ and $\\beta_{1}$ to minimise *RSS*. The minimising value can be shown to be\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2},$$\n",
    "\n",
    "$$\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x},$$\n",
    "\n",
    "where $\\bar{y}$ and $\\bar{x}$ are the sample mean.\n",
    "\n",
    "## Assessing the accuracy of the Coefficient Estimates\n",
    "\n",
    "- The standard error of an estimator reflects how it varies under repeated sampling. We have\n",
    "$$\\text{SE}(\\hat{\\beta_0})^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right],$$\n",
    "$$\\text{SE}(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2},$$\n",
    "where $\\sigma^2 = Var(\\epsilon)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
